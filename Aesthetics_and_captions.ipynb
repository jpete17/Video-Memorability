{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aesthetics and captions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5ZDzHh3sAHGf",
        "5iKKI0oxXkJ4",
        "8v_Gs-1qXur_",
        "BswlHxWXl2lF",
        "0GgDUp636NNn",
        "AGKMpbsW6j7U",
        "9dxLdVwZyfTQ",
        "t5EhFnFJ7U0-",
        "MG14vU7C7Heh"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_bLmwoe--Xq",
        "colab_type": "text"
      },
      "source": [
        "Mount Google drive to Read Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DterKX9Ro6Lj",
        "colab_type": "code",
        "outputId": "8c6b6f9c-ffd3-4e73-b544-35a95dfcc0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ48DA2L-8hv",
        "colab_type": "text"
      },
      "source": [
        "### Method to calcualte spearman's Correlation Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8-5Nwwo7XzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Get_score(Y_pred,Y_true):\n",
        "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
        "    Y_pred = np.squeeze(Y_pred)\n",
        "    Y_true = np.squeeze(Y_true)\n",
        "    if Y_pred.shape != Y_true.shape:\n",
        "        print('Input shapes don\\'t match!')\n",
        "    else:\n",
        "        if len(Y_pred.shape) == 1:\n",
        "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
        "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
        "            print('The Spearman\\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])\n",
        "        else:\n",
        "            for ii in range(Y_pred.shape[1]):\n",
        "                Get_score(Y_pred[:,ii],Y_true[:,ii])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p7ZLeAb_BWA",
        "colab_type": "text"
      },
      "source": [
        "### Import all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKti5Oxpq-sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d60ed2a6-4da1-4f7c-d084-b6f092096767"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools \n",
        "import io\n",
        "\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras import optimizers\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nTS49FebVAq",
        "colab_type": "text"
      },
      "source": [
        "## Aesthetics Mean  - MLP, Random Forest\n",
        "\n",
        "Read the CSV from PC - clean the data i.e remove columns to convert into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_VyI34JrA2K",
        "colab_type": "code",
        "outputId": "a421665a-ff5b-4961-9059-61255787d6c2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-edb33748-544e-42e6-ba73-b96c76f90ab6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-edb33748-544e-42e6-ba73-b96c76f90ab6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving aes_mean.csv to aes_mean (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y0QRgx2rRZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_mean_df = pd.read_csv(io.BytesIO(uploaded['aes_mean.csv']), encoding = \"ISO-8859-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvnjUGs_RG1",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling and cleaning for Aesthetics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qESW7nrd3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_scaled_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzSQRFhYriOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaled_features = aes_mean_df.iloc[:, 1:]\n",
        "column_name = scaled_features.columns\n",
        "x = aes_mean_df.iloc[:, 1:].values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "aes_scaled_df = pd.DataFrame(x_scaled, columns=scaled_features.columns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9bJZ09FuaLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_scaled_df['video_name'] = aes_mean_df[['name']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZkmM2wvWu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = aes_scaled_df.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1]\n",
        "aes_scaled_df = aes_scaled_df[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DXNBUmq0m42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_array = aes_scaled_df.iloc[:, 1: ].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSxbylq1rqeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_scaled_df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht_IwGFs0aT8",
        "colab_type": "code",
        "outputId": "03eb0c43-bb90-471e-fce3-d459c3cda7d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(aes_array.shape, one_hot_res.shape, aes_cap.shape, one_hot_senti.shape, aes_cap_nosenti.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6000, 109) (6000, 4346) (6000, 4456) (6000, 4347) (6000, 4455)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PP8WCvB_jhV",
        "colab_type": "text"
      },
      "source": [
        "Read the labels- long-term and short-term scores for the dependant variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37hRHhR-6lfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_path = './Assignment/Dev-set/Ground-truth/'\n",
        "labels=pd.read_csv(label_path+'ground-truth.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiFXJXa2mwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = aes_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa73bYFF6e4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = labels[['short-term_memorability','long-term_memorability']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITp4WW1f9pZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwgN2FrR6vuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFPbbZ8c_QTY",
        "colab_type": "text"
      },
      "source": [
        "### MultiLayer Perceptron For Aesthetic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvBU50py6zki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bd5bdfa-22f0-4faf-e22a-5260c707a162"
      },
      "source": [
        "model = Sequential()\n",
        "# best - 50,27.2\n",
        "model.add(layers.Dense(50,activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(27,activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "#model.add(layers.Dense(30,activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(2,activation='relu'))\n",
        "\n",
        "# compile the model \n",
        "model.compile(optimizer='adamax',loss='mse',metrics=['accuracy'])\n",
        "# training the model \n",
        "history = model.fit(X_train, Y_train, epochs=50)\n",
        "\n",
        "# load the saved model\n",
        "# evaluate the model\n",
        "# visualizing the model\n",
        "loss = history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss)+1)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.5958\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.6540\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.6742\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.6825\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.6873\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 976us/step - loss: 0.0560 - accuracy: 0.6935\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.6983\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 979us/step - loss: 0.0498 - accuracy: 0.7002\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 988us/step - loss: 0.0471 - accuracy: 0.7033\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.7023\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 958us/step - loss: 0.0423 - accuracy: 0.7035\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 979us/step - loss: 0.0402 - accuracy: 0.7008\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.7035\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.7035\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 991us/step - loss: 0.0341 - accuracy: 0.7033\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 968us/step - loss: 0.0324 - accuracy: 0.7035\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 993us/step - loss: 0.0305 - accuracy: 0.7035\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 0.7035\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.7035\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.7035\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 970us/step - loss: 0.0244 - accuracy: 0.7035\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.7035\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.7035\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.7035\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.7035\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.7035\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.7035\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 959us/step - loss: 0.0182 - accuracy: 0.7035\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 957us/step - loss: 0.0177 - accuracy: 0.7035\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.7035\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 975us/step - loss: 0.0168 - accuracy: 0.7035\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 980us/step - loss: 0.0165 - accuracy: 0.7035\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 998us/step - loss: 0.0163 - accuracy: 0.7035\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 949us/step - loss: 0.0160 - accuracy: 0.7035\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 945us/step - loss: 0.0158 - accuracy: 0.7035\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 970us/step - loss: 0.0156 - accuracy: 0.7035\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 985us/step - loss: 0.0155 - accuracy: 0.7035\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 975us/step - loss: 0.0153 - accuracy: 0.7035\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 990us/step - loss: 0.0152 - accuracy: 0.7035\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 977us/step - loss: 0.0152 - accuracy: 0.7035\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 956us/step - loss: 0.0150 - accuracy: 0.7035\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 962us/step - loss: 0.0150 - accuracy: 0.7035\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 971us/step - loss: 0.0148 - accuracy: 0.7035\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.7035\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 999us/step - loss: 0.0147 - accuracy: 0.7035\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 972us/step - loss: 0.0146 - accuracy: 0.7035\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 984us/step - loss: 0.0145 - accuracy: 0.7035\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 0.7035\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.7035\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 974us/step - loss: 0.0144 - accuracy: 0.7035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AnVXBwO7PVL",
        "colab_type": "code",
        "outputId": "844ac588-01a1-4743-8f01-c1a6e7343ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "plt.figure()\n",
        "acc = history.history['accuracy']\n",
        "#val_acc = history.history['val_acc']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-34bb0fd73a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ6JSCl0_yWR",
        "colab_type": "text"
      },
      "source": [
        "The Spearman's correlation coefficient is: 0.248\n",
        "The Spearman's correlation coefficient is: 0.122"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvZAvQKM7QeN",
        "colab_type": "code",
        "outputId": "2e5fc715-5dc7-45e4-d503-b5f784032021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "predictions = model.predict(X_test)\n",
        "Get_score(predictions, Y_test) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.248\n",
            "The Spearman's correlation coefficient is: 0.122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpgIPiIuCfOE",
        "colab_type": "text"
      },
      "source": [
        "###  Aesthetics Random Forest Model - Training seperately for long term and short term scores\n",
        "\n",
        "A grid search is ran to find the best hyper parameters for both short term and long-term memorability task and a random forest regression is performed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxxh1utB9yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = aes_array\n",
        "#Y = labels[['short-term_memorability']].values\n",
        "Y = labels[['short-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7VsvJS6PaZL",
        "colab_type": "code",
        "outputId": "48f1ab90-9b13-472d-f02c-cf451fe24c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4800, 4348) (1200, 4348) (4800, 1) (1200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCpi7mluLGrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_features': [4, 6, 8],\n",
        "    'min_samples_leaf': [ 4, 6, 8],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [500, 600, 700, 800]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ydj9V2XLzE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "rf = RandomForestRegressor()\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train, Y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9koRptKFOKZ7",
        "colab_type": "code",
        "outputId": "453b971a-4f25-447b-8d04-0371dd6d057a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_features': 8,\n",
              " 'min_samples_leaf': 4,\n",
              " 'min_samples_split': 8,\n",
              " 'n_estimators': 500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG1K6dGM4UFm",
        "colab_type": "text"
      },
      "source": [
        "Aesthetics score for short term memorability - 0.321"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc8qxHtZCdQI",
        "colab_type": "code",
        "outputId": "2be08f67-823a-4384-8630-4eda5796dfb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators=500, max_features= 8, min_samples_leaf = 4, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILnI1rnMhfbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "674715f2-6b93-498b-d36b-0032be060d51"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(Y_test, y_pred) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005532972004171845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJIVr1Xp3UJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = aes_array\n",
        "Y = labels[['long-term_memorability']].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTfhTyv4iSD",
        "colab_type": "text"
      },
      "source": [
        "Aesthetics - long term memorability score is 0.148"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHll-AiECxw",
        "colab_type": "code",
        "outputId": "7e30e07b-032b-4cdf-f872-51db0d9ec956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators=2000, max_features= 5, min_samples_leaf = 3, min_samples_split= 5)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz1C6q6QhlDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c24c458-e0c0-48ec-cbbd-a6f275cde33d"
      },
      "source": [
        "mean_squared_error(Y_test, y_pred) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02133287602399088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZDzHh3sAHGf",
        "colab_type": "text"
      },
      "source": [
        "### Aesthetics with - XGBoost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATaPcIXEESOq",
        "colab_type": "code",
        "outputId": "0e4c8ce6-b611-4f29-c84c-abbab9f0db06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "XGBModel = XGBRegressor(objective ='reg:squarederror',         \n",
        "                 learning_rate=0.1,\n",
        "                 max_depth= 5,\n",
        "                 n_estimators=1000)\n",
        "\n",
        "XGBModel.fit(X_train, Y_train , verbose=False)\n",
        "\n",
        "XGBpredictions = XGBModel.predict(X_test)\n",
        "Get_score(XGBpredictions, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZYtp1zpG-k5",
        "colab_type": "text"
      },
      "source": [
        "### Local Binary Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iKKI0oxXkJ4",
        "colab_type": "text"
      },
      "source": [
        "### To load features from drive and save it as CSV in my local drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkq2gy3nHFVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Method to read HMP'''\n",
        "def read_LBP(fpath, fname, LBP_map):\n",
        "  with open(fpath) as f:\n",
        "    value_list = []\n",
        "    for line in f:\n",
        "      pairs=line.split(' ')\n",
        "      for value in pairs:\n",
        "        value_list.append(value)\n",
        "    LBP_map[fname] = value_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIwxV5uYHOZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_list = []\n",
        "for i in range(1, 7494):\n",
        "  name = 'video' + str(i)\n",
        "  filename = name + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/LBP/' + filename\n",
        "  if(os.path.exists(file_path)):\n",
        "    video_list.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRlC8RwaLLKW",
        "colab_type": "code",
        "outputId": "b9054cfa-d1a3-410b-ed1a-d10aed37006a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(video_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4ak7SMBHV7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_map = {}\n",
        "#hmp_arr = np.empty((0,6075), float)\n",
        "# load the captions\n",
        "for filename in video_list:\n",
        "  filename = filename + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/LBP/' + filename\n",
        "  read_LBP(file_path, filename, LBP_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4uyjPoUNfbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df = pd.DataFrame.from_dict(LBP_map,  orient='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mP776uZNmyc",
        "colab_type": "code",
        "outputId": "129bddaa-9e5e-495e-acd9-c7d09f8dcbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "LBP_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01051071</td>\n",
              "      <td>0.00238281</td>\n",
              "      <td>0.00180411</td>\n",
              "      <td>0.00146991</td>\n",
              "      <td>0.00201582</td>\n",
              "      <td>0.00129726</td>\n",
              "      <td>0.00113715</td>\n",
              "      <td>0.00120033</td>\n",
              "      <td>0.00102575</td>\n",
              "      <td>0.00160397</td>\n",
              "      <td>0.00143663</td>\n",
              "      <td>0.00106192</td>\n",
              "      <td>0.00098042</td>\n",
              "      <td>0.00113088</td>\n",
              "      <td>0.00102816</td>\n",
              "      <td>0.00104408</td>\n",
              "      <td>0.00095390</td>\n",
              "      <td>0.00101273</td>\n",
              "      <td>0.00101611</td>\n",
              "      <td>0.00096402</td>\n",
              "      <td>0.00123119</td>\n",
              "      <td>0.00109471</td>\n",
              "      <td>0.00086709</td>\n",
              "      <td>0.00102286</td>\n",
              "      <td>0.00101514</td>\n",
              "      <td>0.00096740</td>\n",
              "      <td>0.00097367</td>\n",
              "      <td>0.00089988</td>\n",
              "      <td>0.00090278</td>\n",
              "      <td>0.00095390</td>\n",
              "      <td>0.00107591</td>\n",
              "      <td>0.00097849</td>\n",
              "      <td>0.00106530</td>\n",
              "      <td>0.00116609</td>\n",
              "      <td>0.00117284</td>\n",
              "      <td>0.00120563</td>\n",
              "      <td>0.00113956</td>\n",
              "      <td>0.00124904</td>\n",
              "      <td>0.00127749</td>\n",
              "      <td>0.00124035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00152585</td>\n",
              "      <td>0.00168547</td>\n",
              "      <td>0.00132186</td>\n",
              "      <td>0.00190731</td>\n",
              "      <td>0.00131173</td>\n",
              "      <td>0.00170910</td>\n",
              "      <td>0.00156539</td>\n",
              "      <td>0.00138310</td>\n",
              "      <td>0.00177324</td>\n",
              "      <td>0.00170573</td>\n",
              "      <td>0.00143760</td>\n",
              "      <td>0.00158613</td>\n",
              "      <td>0.00179012</td>\n",
              "      <td>0.00184317</td>\n",
              "      <td>0.00183738</td>\n",
              "      <td>0.00265384</td>\n",
              "      <td>0.00191840</td>\n",
              "      <td>0.00165799</td>\n",
              "      <td>0.00194396</td>\n",
              "      <td>0.00253617</td>\n",
              "      <td>0.00194252</td>\n",
              "      <td>0.00229649</td>\n",
              "      <td>0.00226997</td>\n",
              "      <td>0.00298032</td>\n",
              "      <td>0.00263503</td>\n",
              "      <td>0.00293451</td>\n",
              "      <td>0.00548032</td>\n",
              "      <td>0.00467737</td>\n",
              "      <td>0.00420862</td>\n",
              "      <td>0.00448158</td>\n",
              "      <td>0.00403453</td>\n",
              "      <td>0.00712770</td>\n",
              "      <td>0.00781395</td>\n",
              "      <td>0.00612558</td>\n",
              "      <td>0.00747251</td>\n",
              "      <td>0.00762346</td>\n",
              "      <td>0.00648245</td>\n",
              "      <td>0.00406780</td>\n",
              "      <td>0.14867814</td>\n",
              "      <td>0.53409915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01016445</td>\n",
              "      <td>0.00291377</td>\n",
              "      <td>0.00232832</td>\n",
              "      <td>0.00204524</td>\n",
              "      <td>0.00215471</td>\n",
              "      <td>0.00199990</td>\n",
              "      <td>0.00193287</td>\n",
              "      <td>0.00206115</td>\n",
              "      <td>0.00183401</td>\n",
              "      <td>0.00176215</td>\n",
              "      <td>0.00149064</td>\n",
              "      <td>0.00133247</td>\n",
              "      <td>0.00122926</td>\n",
              "      <td>0.00126640</td>\n",
              "      <td>0.00123119</td>\n",
              "      <td>0.00126495</td>\n",
              "      <td>0.00122444</td>\n",
              "      <td>0.00125193</td>\n",
              "      <td>0.00128858</td>\n",
              "      <td>0.00127218</td>\n",
              "      <td>0.00136671</td>\n",
              "      <td>0.00139178</td>\n",
              "      <td>0.00139226</td>\n",
              "      <td>0.00143181</td>\n",
              "      <td>0.00140673</td>\n",
              "      <td>0.00140818</td>\n",
              "      <td>0.00140143</td>\n",
              "      <td>0.00142072</td>\n",
              "      <td>0.00148582</td>\n",
              "      <td>0.00148775</td>\n",
              "      <td>0.00156491</td>\n",
              "      <td>0.00150608</td>\n",
              "      <td>0.00158324</td>\n",
              "      <td>0.00164062</td>\n",
              "      <td>0.00172261</td>\n",
              "      <td>0.00178578</td>\n",
              "      <td>0.00190201</td>\n",
              "      <td>0.00194541</td>\n",
              "      <td>0.00212963</td>\n",
              "      <td>0.00222656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00246576</td>\n",
              "      <td>0.00247733</td>\n",
              "      <td>0.00214265</td>\n",
              "      <td>0.00243248</td>\n",
              "      <td>0.00213301</td>\n",
              "      <td>0.00213349</td>\n",
              "      <td>0.00205440</td>\n",
              "      <td>0.00198544</td>\n",
              "      <td>0.00204909</td>\n",
              "      <td>0.00188561</td>\n",
              "      <td>0.00182726</td>\n",
              "      <td>0.00191937</td>\n",
              "      <td>0.00190008</td>\n",
              "      <td>0.00191069</td>\n",
              "      <td>0.00187452</td>\n",
              "      <td>0.00202353</td>\n",
              "      <td>0.00183690</td>\n",
              "      <td>0.00183546</td>\n",
              "      <td>0.00195843</td>\n",
              "      <td>0.00185812</td>\n",
              "      <td>0.00168403</td>\n",
              "      <td>0.00172936</td>\n",
              "      <td>0.00172020</td>\n",
              "      <td>0.00172164</td>\n",
              "      <td>0.00174817</td>\n",
              "      <td>0.00179929</td>\n",
              "      <td>0.00202884</td>\n",
              "      <td>0.00203125</td>\n",
              "      <td>0.00200087</td>\n",
              "      <td>0.00214747</td>\n",
              "      <td>0.00196277</td>\n",
              "      <td>0.00233989</td>\n",
              "      <td>0.00224585</td>\n",
              "      <td>0.00211564</td>\n",
              "      <td>0.00243634</td>\n",
              "      <td>0.00275752</td>\n",
              "      <td>0.00248505</td>\n",
              "      <td>0.00143856</td>\n",
              "      <td>0.02038484</td>\n",
              "      <td>0.44643181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00749759</td>\n",
              "      <td>0.00117043</td>\n",
              "      <td>0.00096595</td>\n",
              "      <td>0.00072579</td>\n",
              "      <td>0.00120081</td>\n",
              "      <td>0.00070795</td>\n",
              "      <td>0.00074556</td>\n",
              "      <td>0.00086034</td>\n",
              "      <td>0.00076437</td>\n",
              "      <td>0.00124084</td>\n",
              "      <td>0.00113185</td>\n",
              "      <td>0.00075183</td>\n",
              "      <td>0.00066069</td>\n",
              "      <td>0.00067612</td>\n",
              "      <td>0.00057340</td>\n",
              "      <td>0.00073110</td>\n",
              "      <td>0.00057195</td>\n",
              "      <td>0.00058980</td>\n",
              "      <td>0.00058497</td>\n",
              "      <td>0.00057677</td>\n",
              "      <td>0.00116368</td>\n",
              "      <td>0.00105710</td>\n",
              "      <td>0.00071373</td>\n",
              "      <td>0.00098428</td>\n",
              "      <td>0.00097946</td>\n",
              "      <td>0.00080681</td>\n",
              "      <td>0.00088783</td>\n",
              "      <td>0.00090905</td>\n",
              "      <td>0.00078993</td>\n",
              "      <td>0.00088590</td>\n",
              "      <td>0.00105035</td>\n",
              "      <td>0.00087529</td>\n",
              "      <td>0.00104601</td>\n",
              "      <td>0.00120419</td>\n",
              "      <td>0.00115934</td>\n",
              "      <td>0.00119936</td>\n",
              "      <td>0.00118345</td>\n",
              "      <td>0.00138166</td>\n",
              "      <td>0.00152247</td>\n",
              "      <td>0.00140046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00293692</td>\n",
              "      <td>0.00318335</td>\n",
              "      <td>0.00231723</td>\n",
              "      <td>0.00329331</td>\n",
              "      <td>0.00231674</td>\n",
              "      <td>0.00273052</td>\n",
              "      <td>0.00251157</td>\n",
              "      <td>0.00181665</td>\n",
              "      <td>0.00233410</td>\n",
              "      <td>0.00258054</td>\n",
              "      <td>0.00178819</td>\n",
              "      <td>0.00217689</td>\n",
              "      <td>0.00228781</td>\n",
              "      <td>0.00253858</td>\n",
              "      <td>0.00239487</td>\n",
              "      <td>0.00324267</td>\n",
              "      <td>0.00231047</td>\n",
              "      <td>0.00187741</td>\n",
              "      <td>0.00253376</td>\n",
              "      <td>0.00293499</td>\n",
              "      <td>0.00208430</td>\n",
              "      <td>0.00268711</td>\n",
              "      <td>0.00217785</td>\n",
              "      <td>0.00330150</td>\n",
              "      <td>0.00262587</td>\n",
              "      <td>0.00272714</td>\n",
              "      <td>0.00410639</td>\n",
              "      <td>0.00351273</td>\n",
              "      <td>0.00305266</td>\n",
              "      <td>0.00348235</td>\n",
              "      <td>0.00249662</td>\n",
              "      <td>0.00407986</td>\n",
              "      <td>0.00383632</td>\n",
              "      <td>0.00373939</td>\n",
              "      <td>0.00494358</td>\n",
              "      <td>0.00458478</td>\n",
              "      <td>0.00331260</td>\n",
              "      <td>0.00204427</td>\n",
              "      <td>0.13134211</td>\n",
              "      <td>0.48047647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.01338397</td>\n",
              "      <td>0.00459394</td>\n",
              "      <td>0.00335214</td>\n",
              "      <td>0.00266011</td>\n",
              "      <td>0.00246287</td>\n",
              "      <td>0.00210407</td>\n",
              "      <td>0.00187982</td>\n",
              "      <td>0.00178675</td>\n",
              "      <td>0.00173852</td>\n",
              "      <td>0.00149402</td>\n",
              "      <td>0.00114969</td>\n",
              "      <td>0.00090181</td>\n",
              "      <td>0.00069541</td>\n",
              "      <td>0.00068625</td>\n",
              "      <td>0.00054253</td>\n",
              "      <td>0.00054977</td>\n",
              "      <td>0.00043596</td>\n",
              "      <td>0.00041136</td>\n",
              "      <td>0.00036892</td>\n",
              "      <td>0.00036024</td>\n",
              "      <td>0.00073110</td>\n",
              "      <td>0.00058642</td>\n",
              "      <td>0.00026476</td>\n",
              "      <td>0.00042197</td>\n",
              "      <td>0.00041763</td>\n",
              "      <td>0.00027826</td>\n",
              "      <td>0.00025174</td>\n",
              "      <td>0.00026379</td>\n",
              "      <td>0.00023823</td>\n",
              "      <td>0.00021991</td>\n",
              "      <td>0.00030141</td>\n",
              "      <td>0.00027874</td>\n",
              "      <td>0.00034336</td>\n",
              "      <td>0.00037230</td>\n",
              "      <td>0.00035012</td>\n",
              "      <td>0.00036989</td>\n",
              "      <td>0.00032407</td>\n",
              "      <td>0.00037857</td>\n",
              "      <td>0.00039159</td>\n",
              "      <td>0.00037133</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00095486</td>\n",
              "      <td>0.00119743</td>\n",
              "      <td>0.00085696</td>\n",
              "      <td>0.00167728</td>\n",
              "      <td>0.00089940</td>\n",
              "      <td>0.00164255</td>\n",
              "      <td>0.00110725</td>\n",
              "      <td>0.00101659</td>\n",
              "      <td>0.00126591</td>\n",
              "      <td>0.00158999</td>\n",
              "      <td>0.00094666</td>\n",
              "      <td>0.00127170</td>\n",
              "      <td>0.00154948</td>\n",
              "      <td>0.00176408</td>\n",
              "      <td>0.00159626</td>\n",
              "      <td>0.00257234</td>\n",
              "      <td>0.00150559</td>\n",
              "      <td>0.00125965</td>\n",
              "      <td>0.00160012</td>\n",
              "      <td>0.00249180</td>\n",
              "      <td>0.00159770</td>\n",
              "      <td>0.00196663</td>\n",
              "      <td>0.00203897</td>\n",
              "      <td>0.00314911</td>\n",
              "      <td>0.00239776</td>\n",
              "      <td>0.00258198</td>\n",
              "      <td>0.00378617</td>\n",
              "      <td>0.00338976</td>\n",
              "      <td>0.00344088</td>\n",
              "      <td>0.00431424</td>\n",
              "      <td>0.00384018</td>\n",
              "      <td>0.00602527</td>\n",
              "      <td>0.00576437</td>\n",
              "      <td>0.00666329</td>\n",
              "      <td>0.01064863</td>\n",
              "      <td>0.00996721</td>\n",
              "      <td>0.00863812</td>\n",
              "      <td>0.00508295</td>\n",
              "      <td>0.17544898</td>\n",
              "      <td>0.60352189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.02373216</td>\n",
              "      <td>0.00531973</td>\n",
              "      <td>0.00533999</td>\n",
              "      <td>0.00461806</td>\n",
              "      <td>0.00399643</td>\n",
              "      <td>0.00348428</td>\n",
              "      <td>0.00309028</td>\n",
              "      <td>0.00296682</td>\n",
              "      <td>0.00278791</td>\n",
              "      <td>0.00264709</td>\n",
              "      <td>0.00274691</td>\n",
              "      <td>0.00263648</td>\n",
              "      <td>0.00258584</td>\n",
              "      <td>0.00253713</td>\n",
              "      <td>0.00242139</td>\n",
              "      <td>0.00234423</td>\n",
              "      <td>0.00225887</td>\n",
              "      <td>0.00210503</td>\n",
              "      <td>0.00196856</td>\n",
              "      <td>0.00186535</td>\n",
              "      <td>0.00164207</td>\n",
              "      <td>0.00152730</td>\n",
              "      <td>0.00136815</td>\n",
              "      <td>0.00123843</td>\n",
              "      <td>0.00111304</td>\n",
              "      <td>0.00096547</td>\n",
              "      <td>0.00090664</td>\n",
              "      <td>0.00086806</td>\n",
              "      <td>0.00078848</td>\n",
              "      <td>0.00076726</td>\n",
              "      <td>0.00076051</td>\n",
              "      <td>0.00076534</td>\n",
              "      <td>0.00076437</td>\n",
              "      <td>0.00079234</td>\n",
              "      <td>0.00084635</td>\n",
              "      <td>0.00086323</td>\n",
              "      <td>0.00083671</td>\n",
              "      <td>0.00090374</td>\n",
              "      <td>0.00094907</td>\n",
              "      <td>0.00098380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00087722</td>\n",
              "      <td>0.00093557</td>\n",
              "      <td>0.00082079</td>\n",
              "      <td>0.00089651</td>\n",
              "      <td>0.00081549</td>\n",
              "      <td>0.00087722</td>\n",
              "      <td>0.00081019</td>\n",
              "      <td>0.00087818</td>\n",
              "      <td>0.00081694</td>\n",
              "      <td>0.00087674</td>\n",
              "      <td>0.00087143</td>\n",
              "      <td>0.00095534</td>\n",
              "      <td>0.00096981</td>\n",
              "      <td>0.00110340</td>\n",
              "      <td>0.00114439</td>\n",
              "      <td>0.00132137</td>\n",
              "      <td>0.00141975</td>\n",
              "      <td>0.00163532</td>\n",
              "      <td>0.00173225</td>\n",
              "      <td>0.00197049</td>\n",
              "      <td>0.00207514</td>\n",
              "      <td>0.00216339</td>\n",
              "      <td>0.00232832</td>\n",
              "      <td>0.00248216</td>\n",
              "      <td>0.00264178</td>\n",
              "      <td>0.00278694</td>\n",
              "      <td>0.00297454</td>\n",
              "      <td>0.00313465</td>\n",
              "      <td>0.00305556</td>\n",
              "      <td>0.00300106</td>\n",
              "      <td>0.00315779</td>\n",
              "      <td>0.00317178</td>\n",
              "      <td>0.00319348</td>\n",
              "      <td>0.00354504</td>\n",
              "      <td>0.00372155</td>\n",
              "      <td>0.00444203</td>\n",
              "      <td>0.00485725</td>\n",
              "      <td>0.00410060</td>\n",
              "      <td>0.02763841</td>\n",
              "      <td>0.69043113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0           1           2    ...         119         120         121\n",
              "0  0.01051071  0.00238281  0.00180411  ...  0.00406780  0.14867814  0.53409915\n",
              "1  0.01016445  0.00291377  0.00232832  ...  0.00143856  0.02038484  0.44643181\n",
              "2  0.00749759  0.00117043  0.00096595  ...  0.00204427  0.13134211  0.48047647\n",
              "3  0.01338397  0.00459394  0.00335214  ...  0.00508295  0.17544898  0.60352189\n",
              "4  0.02373216  0.00531973  0.00533999  ...  0.00410060  0.02763841  0.69043113\n",
              "\n",
              "[5 rows x 122 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekLeBIWzHY4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_list = []\n",
        "for i in range(1, 123):\n",
        "  cname = 'lbp' + str(i)\n",
        "  column_list.append(cname)\n",
        "\n",
        "LBP_df.columns = column_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUj6Zwq6HbNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkLE3awoqVtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df.drop(columns=['index'], inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR9ePHJ6HfGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df = LBP_df.iloc[:, : -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdPd32x3mM3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df.to_csv('/content/drive/My Drive/LBP_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7XifVB4dNeB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v_Gs-1qXur_",
        "colab_type": "text"
      },
      "source": [
        "### Testing with LBP features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yv78TEdRm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df = pd.read_csv('LBP.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7FlidtldZxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df = LBP_df.iloc[: , 2:]\n",
        "LBP_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG8QkPMzcWdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df = LBP_df.astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoF93rsWctMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LBP_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56tnOQ4lf1cQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aes_lbp = np.concatenate((aes_array, LBP_df.values), axis=1)\n",
        "lbp_hmp = np.concatenate((hmp_array, LBP_df.values), axis=1)\n",
        "lbp_one_hot_res = np.concatenate((one_hot_res, LBP_df.values), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh1EMIYbay0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = lbp_hmp\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['long-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qXTsjLFhEDe",
        "colab_type": "code",
        "outputId": "b6025bfb-16ac-4cd6-bcf2-61276e8d9d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4800, 231) (1200, 231) (4800, 1) (1200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjohgWlfbglI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cab6cc7a-8105-459c-c32d-4c4feb0fe231"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators=1000, max_features= 8, min_samples_leaf = 4, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SSM2MsyhhmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "# best - 50,27.2\n",
        "model.add(layers.Dense(50,activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(27,activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "#model.add(layers.Dense(30,activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(2,activation='relu'))\n",
        "\n",
        "# compile the model \n",
        "model.compile(optimizer='adamax',loss='mse',metrics=['accuracy'])\n",
        "\n",
        "# training the model \n",
        "history = model.fit(X_train, Y_train, epochs=20)\n",
        "\n",
        "# visualizing the model\n",
        "loss = history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss)+1)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tq_D7J9h1jb",
        "colab_type": "code",
        "outputId": "3471f90f-db17-4233-ed80-b3e3e0a8a0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.241\n",
            "The Spearman's correlation coefficient is: 0.122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BswlHxWXl2lF",
        "colab_type": "text"
      },
      "source": [
        "### COLOR HISTOGRAM - Reading the middle layer of each file\n",
        " Reading data from cloud file and storing it in a dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr72W6gcl140",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Method to read HMP'''\n",
        "def read_CH(fpath, fname, CH_map):\n",
        "    with open(fpath) as f:\n",
        "      for line in f:\n",
        "        pairs=line.split()\n",
        "        CH_temp = { int(p.split(':')[0]) : float(p.split(':')[1]) for p in pairs}\n",
        "    # there are 6075 bins, fill zeros\n",
        "    CH = np.zeros(256)\n",
        "    for idx in CH_temp.keys():\n",
        "        CH[idx-1] = CH_temp[idx]            \n",
        "    return CH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohuWYEnQotCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_listch = []\n",
        "for i in range(1, 7494):\n",
        "  name = 'video' + str(i)\n",
        "  filename = name + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/ColorHistogram/' + filename\n",
        "  if(os.path.exists(file_path)):\n",
        "    video_listch.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hvp-_20uwxX",
        "colab_type": "code",
        "outputId": "e64358ae-7526-41f3-c59b-3fd5df055aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(video_listch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWg3epI0mli_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CH_map = {}\n",
        "CH_arr = np.empty((0,256), float)\n",
        "\n",
        "for filename in video_list:\n",
        "  filename = filename + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/ColorHistogram/' + filename\n",
        "  CH = read_CH(file_path, filename, CH_map)\n",
        "  CH_arr = np.vstack([CH_arr, CH])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-jZ3vrmVTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_list = []\n",
        "for i in range(1, 257):\n",
        "  cname = 'CH' + str(i)\n",
        "  column_list.append(cname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2UZvagN0KhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ch_df = pd.DataFrame(CH_arr, columns=column_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpqD0A4gzb8W",
        "colab_type": "code",
        "outputId": "340a2a4b-5d4a-42c9-d1a1-567ea16dda62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "ch_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CH1</th>\n",
              "      <th>CH2</th>\n",
              "      <th>CH3</th>\n",
              "      <th>CH4</th>\n",
              "      <th>CH5</th>\n",
              "      <th>CH6</th>\n",
              "      <th>CH7</th>\n",
              "      <th>CH8</th>\n",
              "      <th>CH9</th>\n",
              "      <th>CH10</th>\n",
              "      <th>CH11</th>\n",
              "      <th>CH12</th>\n",
              "      <th>CH13</th>\n",
              "      <th>CH14</th>\n",
              "      <th>CH15</th>\n",
              "      <th>CH16</th>\n",
              "      <th>CH17</th>\n",
              "      <th>CH18</th>\n",
              "      <th>CH19</th>\n",
              "      <th>CH20</th>\n",
              "      <th>CH21</th>\n",
              "      <th>CH22</th>\n",
              "      <th>CH23</th>\n",
              "      <th>CH24</th>\n",
              "      <th>CH25</th>\n",
              "      <th>CH26</th>\n",
              "      <th>CH27</th>\n",
              "      <th>CH28</th>\n",
              "      <th>CH29</th>\n",
              "      <th>CH30</th>\n",
              "      <th>CH31</th>\n",
              "      <th>CH32</th>\n",
              "      <th>CH33</th>\n",
              "      <th>CH34</th>\n",
              "      <th>CH35</th>\n",
              "      <th>CH36</th>\n",
              "      <th>CH37</th>\n",
              "      <th>CH38</th>\n",
              "      <th>CH39</th>\n",
              "      <th>CH40</th>\n",
              "      <th>...</th>\n",
              "      <th>CH217</th>\n",
              "      <th>CH218</th>\n",
              "      <th>CH219</th>\n",
              "      <th>CH220</th>\n",
              "      <th>CH221</th>\n",
              "      <th>CH222</th>\n",
              "      <th>CH223</th>\n",
              "      <th>CH224</th>\n",
              "      <th>CH225</th>\n",
              "      <th>CH226</th>\n",
              "      <th>CH227</th>\n",
              "      <th>CH228</th>\n",
              "      <th>CH229</th>\n",
              "      <th>CH230</th>\n",
              "      <th>CH231</th>\n",
              "      <th>CH232</th>\n",
              "      <th>CH233</th>\n",
              "      <th>CH234</th>\n",
              "      <th>CH235</th>\n",
              "      <th>CH236</th>\n",
              "      <th>CH237</th>\n",
              "      <th>CH238</th>\n",
              "      <th>CH239</th>\n",
              "      <th>CH240</th>\n",
              "      <th>CH241</th>\n",
              "      <th>CH242</th>\n",
              "      <th>CH243</th>\n",
              "      <th>CH244</th>\n",
              "      <th>CH245</th>\n",
              "      <th>CH246</th>\n",
              "      <th>CH247</th>\n",
              "      <th>CH248</th>\n",
              "      <th>CH249</th>\n",
              "      <th>CH250</th>\n",
              "      <th>CH251</th>\n",
              "      <th>CH252</th>\n",
              "      <th>CH253</th>\n",
              "      <th>CH254</th>\n",
              "      <th>CH255</th>\n",
              "      <th>CH256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>66131.0</td>\n",
              "      <td>61148.0</td>\n",
              "      <td>231869.0</td>\n",
              "      <td>49885.0</td>\n",
              "      <td>14711.0</td>\n",
              "      <td>96686.0</td>\n",
              "      <td>8920.0</td>\n",
              "      <td>14507.0</td>\n",
              "      <td>19699.0</td>\n",
              "      <td>9596.0</td>\n",
              "      <td>9686.0</td>\n",
              "      <td>16456.0</td>\n",
              "      <td>20635.0</td>\n",
              "      <td>8540.0</td>\n",
              "      <td>33913.0</td>\n",
              "      <td>8448.0</td>\n",
              "      <td>17803.0</td>\n",
              "      <td>22303.0</td>\n",
              "      <td>25465.0</td>\n",
              "      <td>14213.0</td>\n",
              "      <td>107114.0</td>\n",
              "      <td>111482.0</td>\n",
              "      <td>45762.0</td>\n",
              "      <td>22372.0</td>\n",
              "      <td>64597.0</td>\n",
              "      <td>57173.0</td>\n",
              "      <td>18299.0</td>\n",
              "      <td>28792.0</td>\n",
              "      <td>14359.0</td>\n",
              "      <td>8669.0</td>\n",
              "      <td>6060.0</td>\n",
              "      <td>9581.0</td>\n",
              "      <td>6617.0</td>\n",
              "      <td>7335.0</td>\n",
              "      <td>7058.0</td>\n",
              "      <td>6161.0</td>\n",
              "      <td>6290.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8553.0</td>\n",
              "      <td>3664.0</td>\n",
              "      <td>3099.0</td>\n",
              "      <td>3054.0</td>\n",
              "      <td>2642.0</td>\n",
              "      <td>2833.0</td>\n",
              "      <td>2355.0</td>\n",
              "      <td>2389.0</td>\n",
              "      <td>2419.0</td>\n",
              "      <td>2164.0</td>\n",
              "      <td>2613.0</td>\n",
              "      <td>2698.0</td>\n",
              "      <td>2867.0</td>\n",
              "      <td>2926.0</td>\n",
              "      <td>2578.0</td>\n",
              "      <td>3165.0</td>\n",
              "      <td>2417.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>3425.0</td>\n",
              "      <td>3166.0</td>\n",
              "      <td>2991.0</td>\n",
              "      <td>3264.0</td>\n",
              "      <td>3979.0</td>\n",
              "      <td>3725.0</td>\n",
              "      <td>3733.0</td>\n",
              "      <td>3797.0</td>\n",
              "      <td>3670.0</td>\n",
              "      <td>3639.0</td>\n",
              "      <td>4045.0</td>\n",
              "      <td>4733.0</td>\n",
              "      <td>4040.0</td>\n",
              "      <td>3903.0</td>\n",
              "      <td>3905.0</td>\n",
              "      <td>4519.0</td>\n",
              "      <td>3702.0</td>\n",
              "      <td>4937.0</td>\n",
              "      <td>5229.0</td>\n",
              "      <td>6037.0</td>\n",
              "      <td>4305.0</td>\n",
              "      <td>4329.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1762.0</td>\n",
              "      <td>1834.0</td>\n",
              "      <td>1086.0</td>\n",
              "      <td>1658.0</td>\n",
              "      <td>2204.0</td>\n",
              "      <td>2297.0</td>\n",
              "      <td>2446.0</td>\n",
              "      <td>1707.0</td>\n",
              "      <td>1663.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>1459.0</td>\n",
              "      <td>1461.0</td>\n",
              "      <td>1415.0</td>\n",
              "      <td>1401.0</td>\n",
              "      <td>1343.0</td>\n",
              "      <td>1228.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>991.0</td>\n",
              "      <td>1263.0</td>\n",
              "      <td>1491.0</td>\n",
              "      <td>2061.0</td>\n",
              "      <td>1305.0</td>\n",
              "      <td>979.0</td>\n",
              "      <td>462.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>1499.0</td>\n",
              "      <td>1476.0</td>\n",
              "      <td>2647.0</td>\n",
              "      <td>1579.0</td>\n",
              "      <td>1294.0</td>\n",
              "      <td>464.0</td>\n",
              "      <td>1190.0</td>\n",
              "      <td>1890.0</td>\n",
              "      <td>1607.0</td>\n",
              "      <td>3050.0</td>\n",
              "      <td>1850.0</td>\n",
              "      <td>4213.0</td>\n",
              "      <td>511.0</td>\n",
              "      <td>9524.0</td>\n",
              "      <td>31480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5696.0</td>\n",
              "      <td>91903.0</td>\n",
              "      <td>80186.0</td>\n",
              "      <td>37287.0</td>\n",
              "      <td>44852.0</td>\n",
              "      <td>36775.0</td>\n",
              "      <td>41904.0</td>\n",
              "      <td>27869.0</td>\n",
              "      <td>25416.0</td>\n",
              "      <td>22236.0</td>\n",
              "      <td>18316.0</td>\n",
              "      <td>39323.0</td>\n",
              "      <td>15505.0</td>\n",
              "      <td>26902.0</td>\n",
              "      <td>41416.0</td>\n",
              "      <td>30495.0</td>\n",
              "      <td>19845.0</td>\n",
              "      <td>22148.0</td>\n",
              "      <td>20832.0</td>\n",
              "      <td>21224.0</td>\n",
              "      <td>14633.0</td>\n",
              "      <td>25283.0</td>\n",
              "      <td>18046.0</td>\n",
              "      <td>16818.0</td>\n",
              "      <td>15979.0</td>\n",
              "      <td>12069.0</td>\n",
              "      <td>23046.0</td>\n",
              "      <td>13225.0</td>\n",
              "      <td>18161.0</td>\n",
              "      <td>21911.0</td>\n",
              "      <td>17493.0</td>\n",
              "      <td>17426.0</td>\n",
              "      <td>12047.0</td>\n",
              "      <td>18392.0</td>\n",
              "      <td>15011.0</td>\n",
              "      <td>23790.0</td>\n",
              "      <td>17894.0</td>\n",
              "      <td>39308.0</td>\n",
              "      <td>25596.0</td>\n",
              "      <td>16867.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>447.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>403.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>297.0</td>\n",
              "      <td>341.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>332.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>328.0</td>\n",
              "      <td>278.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>246.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1473.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>1232.0</td>\n",
              "      <td>1227.0</td>\n",
              "      <td>1380.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>1416.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>1184.0</td>\n",
              "      <td>1285.0</td>\n",
              "      <td>1118.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>1232.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1199.0</td>\n",
              "      <td>1316.0</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>1252.0</td>\n",
              "      <td>1157.0</td>\n",
              "      <td>1528.0</td>\n",
              "      <td>1581.0</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>1562.0</td>\n",
              "      <td>1657.0</td>\n",
              "      <td>1601.0</td>\n",
              "      <td>1430.0</td>\n",
              "      <td>1964.0</td>\n",
              "      <td>2289.0</td>\n",
              "      <td>2255.0</td>\n",
              "      <td>2751.0</td>\n",
              "      <td>2444.0</td>\n",
              "      <td>3692.0</td>\n",
              "      <td>1093.0</td>\n",
              "      <td>4477.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9582.0</td>\n",
              "      <td>8560.0</td>\n",
              "      <td>9828.0</td>\n",
              "      <td>8988.0</td>\n",
              "      <td>9273.0</td>\n",
              "      <td>7578.0</td>\n",
              "      <td>7139.0</td>\n",
              "      <td>8469.0</td>\n",
              "      <td>8453.0</td>\n",
              "      <td>7654.0</td>\n",
              "      <td>7368.0</td>\n",
              "      <td>8017.0</td>\n",
              "      <td>6834.0</td>\n",
              "      <td>5747.0</td>\n",
              "      <td>6703.0</td>\n",
              "      <td>5224.0</td>\n",
              "      <td>5810.0</td>\n",
              "      <td>5356.0</td>\n",
              "      <td>5238.0</td>\n",
              "      <td>4777.0</td>\n",
              "      <td>4217.0</td>\n",
              "      <td>5022.0</td>\n",
              "      <td>4101.0</td>\n",
              "      <td>4265.0</td>\n",
              "      <td>3980.0</td>\n",
              "      <td>3679.0</td>\n",
              "      <td>3938.0</td>\n",
              "      <td>3574.0</td>\n",
              "      <td>3687.0</td>\n",
              "      <td>3340.0</td>\n",
              "      <td>3266.0</td>\n",
              "      <td>3356.0</td>\n",
              "      <td>3108.0</td>\n",
              "      <td>3459.0</td>\n",
              "      <td>3312.0</td>\n",
              "      <td>3271.0</td>\n",
              "      <td>3502.0</td>\n",
              "      <td>3398.0</td>\n",
              "      <td>3629.0</td>\n",
              "      <td>3286.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3159.0</td>\n",
              "      <td>3138.0</td>\n",
              "      <td>3264.0</td>\n",
              "      <td>3462.0</td>\n",
              "      <td>3411.0</td>\n",
              "      <td>3674.0</td>\n",
              "      <td>3638.0</td>\n",
              "      <td>3933.0</td>\n",
              "      <td>4019.0</td>\n",
              "      <td>4276.0</td>\n",
              "      <td>4433.0</td>\n",
              "      <td>4498.0</td>\n",
              "      <td>4877.0</td>\n",
              "      <td>5041.0</td>\n",
              "      <td>5160.0</td>\n",
              "      <td>5460.0</td>\n",
              "      <td>5795.0</td>\n",
              "      <td>5969.0</td>\n",
              "      <td>6457.0</td>\n",
              "      <td>6852.0</td>\n",
              "      <td>7245.0</td>\n",
              "      <td>7753.0</td>\n",
              "      <td>8259.0</td>\n",
              "      <td>8563.0</td>\n",
              "      <td>9075.0</td>\n",
              "      <td>9916.0</td>\n",
              "      <td>10800.0</td>\n",
              "      <td>11420.0</td>\n",
              "      <td>12543.0</td>\n",
              "      <td>13381.0</td>\n",
              "      <td>13817.0</td>\n",
              "      <td>16118.0</td>\n",
              "      <td>16545.0</td>\n",
              "      <td>16434.0</td>\n",
              "      <td>19663.0</td>\n",
              "      <td>17034.0</td>\n",
              "      <td>21174.0</td>\n",
              "      <td>17200.0</td>\n",
              "      <td>139598.0</td>\n",
              "      <td>39228.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 256 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CH1      CH2      CH3      CH4  ...    CH253    CH254     CH255    CH256\n",
              "0     3.0    285.0    331.0  66131.0  ...      0.0      0.0       4.0      0.0\n",
              "1  8553.0   3664.0   3099.0   3054.0  ...   4213.0    511.0    9524.0  31480.0\n",
              "2  5696.0  91903.0  80186.0  37287.0  ...      0.0      0.0       0.0    447.0\n",
              "3     0.0      0.0      0.0      0.0  ...   3692.0   1093.0    4477.0      0.0\n",
              "4  9582.0   8560.0   9828.0   8988.0  ...  21174.0  17200.0  139598.0  39228.0\n",
              "\n",
              "[5 rows x 256 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK8Xk3AmzXJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ch_df.to_csv('/content/drive/My Drive/ch_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esj9X15Senh_",
        "colab_type": "text"
      },
      "source": [
        "### Color Hist - Run Random forest test for short-term and long-term scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCo3etyCesrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ch_df = pd.read_csv('ch_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPpSLb6Re46u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "70892d40-99f4-49f0-b582-9a07c1fc009f"
      },
      "source": [
        "ch_df = ch_df.iloc[: , 1:]\n",
        "ch_df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CH1</th>\n",
              "      <th>CH2</th>\n",
              "      <th>CH3</th>\n",
              "      <th>CH4</th>\n",
              "      <th>CH5</th>\n",
              "      <th>CH6</th>\n",
              "      <th>CH7</th>\n",
              "      <th>CH8</th>\n",
              "      <th>CH9</th>\n",
              "      <th>CH10</th>\n",
              "      <th>CH11</th>\n",
              "      <th>CH12</th>\n",
              "      <th>CH13</th>\n",
              "      <th>CH14</th>\n",
              "      <th>CH15</th>\n",
              "      <th>CH16</th>\n",
              "      <th>CH17</th>\n",
              "      <th>CH18</th>\n",
              "      <th>CH19</th>\n",
              "      <th>CH20</th>\n",
              "      <th>CH21</th>\n",
              "      <th>CH22</th>\n",
              "      <th>CH23</th>\n",
              "      <th>CH24</th>\n",
              "      <th>CH25</th>\n",
              "      <th>CH26</th>\n",
              "      <th>CH27</th>\n",
              "      <th>CH28</th>\n",
              "      <th>CH29</th>\n",
              "      <th>CH30</th>\n",
              "      <th>CH31</th>\n",
              "      <th>CH32</th>\n",
              "      <th>CH33</th>\n",
              "      <th>CH34</th>\n",
              "      <th>CH35</th>\n",
              "      <th>CH36</th>\n",
              "      <th>CH37</th>\n",
              "      <th>CH38</th>\n",
              "      <th>CH39</th>\n",
              "      <th>CH40</th>\n",
              "      <th>...</th>\n",
              "      <th>CH217</th>\n",
              "      <th>CH218</th>\n",
              "      <th>CH219</th>\n",
              "      <th>CH220</th>\n",
              "      <th>CH221</th>\n",
              "      <th>CH222</th>\n",
              "      <th>CH223</th>\n",
              "      <th>CH224</th>\n",
              "      <th>CH225</th>\n",
              "      <th>CH226</th>\n",
              "      <th>CH227</th>\n",
              "      <th>CH228</th>\n",
              "      <th>CH229</th>\n",
              "      <th>CH230</th>\n",
              "      <th>CH231</th>\n",
              "      <th>CH232</th>\n",
              "      <th>CH233</th>\n",
              "      <th>CH234</th>\n",
              "      <th>CH235</th>\n",
              "      <th>CH236</th>\n",
              "      <th>CH237</th>\n",
              "      <th>CH238</th>\n",
              "      <th>CH239</th>\n",
              "      <th>CH240</th>\n",
              "      <th>CH241</th>\n",
              "      <th>CH242</th>\n",
              "      <th>CH243</th>\n",
              "      <th>CH244</th>\n",
              "      <th>CH245</th>\n",
              "      <th>CH246</th>\n",
              "      <th>CH247</th>\n",
              "      <th>CH248</th>\n",
              "      <th>CH249</th>\n",
              "      <th>CH250</th>\n",
              "      <th>CH251</th>\n",
              "      <th>CH252</th>\n",
              "      <th>CH253</th>\n",
              "      <th>CH254</th>\n",
              "      <th>CH255</th>\n",
              "      <th>CH256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>66131.0</td>\n",
              "      <td>61148.0</td>\n",
              "      <td>231869.0</td>\n",
              "      <td>49885.0</td>\n",
              "      <td>14711.0</td>\n",
              "      <td>96686.0</td>\n",
              "      <td>8920.0</td>\n",
              "      <td>14507.0</td>\n",
              "      <td>19699.0</td>\n",
              "      <td>9596.0</td>\n",
              "      <td>9686.0</td>\n",
              "      <td>16456.0</td>\n",
              "      <td>20635.0</td>\n",
              "      <td>8540.0</td>\n",
              "      <td>33913.0</td>\n",
              "      <td>8448.0</td>\n",
              "      <td>17803.0</td>\n",
              "      <td>22303.0</td>\n",
              "      <td>25465.0</td>\n",
              "      <td>14213.0</td>\n",
              "      <td>107114.0</td>\n",
              "      <td>111482.0</td>\n",
              "      <td>45762.0</td>\n",
              "      <td>22372.0</td>\n",
              "      <td>64597.0</td>\n",
              "      <td>57173.0</td>\n",
              "      <td>18299.0</td>\n",
              "      <td>28792.0</td>\n",
              "      <td>14359.0</td>\n",
              "      <td>8669.0</td>\n",
              "      <td>6060.0</td>\n",
              "      <td>9581.0</td>\n",
              "      <td>6617.0</td>\n",
              "      <td>7335.0</td>\n",
              "      <td>7058.0</td>\n",
              "      <td>6161.0</td>\n",
              "      <td>6290.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8553.0</td>\n",
              "      <td>3664.0</td>\n",
              "      <td>3099.0</td>\n",
              "      <td>3054.0</td>\n",
              "      <td>2642.0</td>\n",
              "      <td>2833.0</td>\n",
              "      <td>2355.0</td>\n",
              "      <td>2389.0</td>\n",
              "      <td>2419.0</td>\n",
              "      <td>2164.0</td>\n",
              "      <td>2613.0</td>\n",
              "      <td>2698.0</td>\n",
              "      <td>2867.0</td>\n",
              "      <td>2926.0</td>\n",
              "      <td>2578.0</td>\n",
              "      <td>3165.0</td>\n",
              "      <td>2417.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>3425.0</td>\n",
              "      <td>3166.0</td>\n",
              "      <td>2991.0</td>\n",
              "      <td>3264.0</td>\n",
              "      <td>3979.0</td>\n",
              "      <td>3725.0</td>\n",
              "      <td>3733.0</td>\n",
              "      <td>3797.0</td>\n",
              "      <td>3670.0</td>\n",
              "      <td>3639.0</td>\n",
              "      <td>4045.0</td>\n",
              "      <td>4733.0</td>\n",
              "      <td>4040.0</td>\n",
              "      <td>3903.0</td>\n",
              "      <td>3905.0</td>\n",
              "      <td>4519.0</td>\n",
              "      <td>3702.0</td>\n",
              "      <td>4937.0</td>\n",
              "      <td>5229.0</td>\n",
              "      <td>6037.0</td>\n",
              "      <td>4305.0</td>\n",
              "      <td>4329.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1762.0</td>\n",
              "      <td>1834.0</td>\n",
              "      <td>1086.0</td>\n",
              "      <td>1658.0</td>\n",
              "      <td>2204.0</td>\n",
              "      <td>2297.0</td>\n",
              "      <td>2446.0</td>\n",
              "      <td>1707.0</td>\n",
              "      <td>1663.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>1459.0</td>\n",
              "      <td>1461.0</td>\n",
              "      <td>1415.0</td>\n",
              "      <td>1401.0</td>\n",
              "      <td>1343.0</td>\n",
              "      <td>1228.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>991.0</td>\n",
              "      <td>1263.0</td>\n",
              "      <td>1491.0</td>\n",
              "      <td>2061.0</td>\n",
              "      <td>1305.0</td>\n",
              "      <td>979.0</td>\n",
              "      <td>462.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>1499.0</td>\n",
              "      <td>1476.0</td>\n",
              "      <td>2647.0</td>\n",
              "      <td>1579.0</td>\n",
              "      <td>1294.0</td>\n",
              "      <td>464.0</td>\n",
              "      <td>1190.0</td>\n",
              "      <td>1890.0</td>\n",
              "      <td>1607.0</td>\n",
              "      <td>3050.0</td>\n",
              "      <td>1850.0</td>\n",
              "      <td>4213.0</td>\n",
              "      <td>511.0</td>\n",
              "      <td>9524.0</td>\n",
              "      <td>31480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5696.0</td>\n",
              "      <td>91903.0</td>\n",
              "      <td>80186.0</td>\n",
              "      <td>37287.0</td>\n",
              "      <td>44852.0</td>\n",
              "      <td>36775.0</td>\n",
              "      <td>41904.0</td>\n",
              "      <td>27869.0</td>\n",
              "      <td>25416.0</td>\n",
              "      <td>22236.0</td>\n",
              "      <td>18316.0</td>\n",
              "      <td>39323.0</td>\n",
              "      <td>15505.0</td>\n",
              "      <td>26902.0</td>\n",
              "      <td>41416.0</td>\n",
              "      <td>30495.0</td>\n",
              "      <td>19845.0</td>\n",
              "      <td>22148.0</td>\n",
              "      <td>20832.0</td>\n",
              "      <td>21224.0</td>\n",
              "      <td>14633.0</td>\n",
              "      <td>25283.0</td>\n",
              "      <td>18046.0</td>\n",
              "      <td>16818.0</td>\n",
              "      <td>15979.0</td>\n",
              "      <td>12069.0</td>\n",
              "      <td>23046.0</td>\n",
              "      <td>13225.0</td>\n",
              "      <td>18161.0</td>\n",
              "      <td>21911.0</td>\n",
              "      <td>17493.0</td>\n",
              "      <td>17426.0</td>\n",
              "      <td>12047.0</td>\n",
              "      <td>18392.0</td>\n",
              "      <td>15011.0</td>\n",
              "      <td>23790.0</td>\n",
              "      <td>17894.0</td>\n",
              "      <td>39308.0</td>\n",
              "      <td>25596.0</td>\n",
              "      <td>16867.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>447.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>403.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>297.0</td>\n",
              "      <td>341.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>332.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>328.0</td>\n",
              "      <td>278.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>246.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1473.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>1232.0</td>\n",
              "      <td>1227.0</td>\n",
              "      <td>1380.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>1416.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>1184.0</td>\n",
              "      <td>1285.0</td>\n",
              "      <td>1118.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>1232.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1199.0</td>\n",
              "      <td>1316.0</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>1252.0</td>\n",
              "      <td>1157.0</td>\n",
              "      <td>1528.0</td>\n",
              "      <td>1581.0</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>1562.0</td>\n",
              "      <td>1657.0</td>\n",
              "      <td>1601.0</td>\n",
              "      <td>1430.0</td>\n",
              "      <td>1964.0</td>\n",
              "      <td>2289.0</td>\n",
              "      <td>2255.0</td>\n",
              "      <td>2751.0</td>\n",
              "      <td>2444.0</td>\n",
              "      <td>3692.0</td>\n",
              "      <td>1093.0</td>\n",
              "      <td>4477.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9582.0</td>\n",
              "      <td>8560.0</td>\n",
              "      <td>9828.0</td>\n",
              "      <td>8988.0</td>\n",
              "      <td>9273.0</td>\n",
              "      <td>7578.0</td>\n",
              "      <td>7139.0</td>\n",
              "      <td>8469.0</td>\n",
              "      <td>8453.0</td>\n",
              "      <td>7654.0</td>\n",
              "      <td>7368.0</td>\n",
              "      <td>8017.0</td>\n",
              "      <td>6834.0</td>\n",
              "      <td>5747.0</td>\n",
              "      <td>6703.0</td>\n",
              "      <td>5224.0</td>\n",
              "      <td>5810.0</td>\n",
              "      <td>5356.0</td>\n",
              "      <td>5238.0</td>\n",
              "      <td>4777.0</td>\n",
              "      <td>4217.0</td>\n",
              "      <td>5022.0</td>\n",
              "      <td>4101.0</td>\n",
              "      <td>4265.0</td>\n",
              "      <td>3980.0</td>\n",
              "      <td>3679.0</td>\n",
              "      <td>3938.0</td>\n",
              "      <td>3574.0</td>\n",
              "      <td>3687.0</td>\n",
              "      <td>3340.0</td>\n",
              "      <td>3266.0</td>\n",
              "      <td>3356.0</td>\n",
              "      <td>3108.0</td>\n",
              "      <td>3459.0</td>\n",
              "      <td>3312.0</td>\n",
              "      <td>3271.0</td>\n",
              "      <td>3502.0</td>\n",
              "      <td>3398.0</td>\n",
              "      <td>3629.0</td>\n",
              "      <td>3286.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3159.0</td>\n",
              "      <td>3138.0</td>\n",
              "      <td>3264.0</td>\n",
              "      <td>3462.0</td>\n",
              "      <td>3411.0</td>\n",
              "      <td>3674.0</td>\n",
              "      <td>3638.0</td>\n",
              "      <td>3933.0</td>\n",
              "      <td>4019.0</td>\n",
              "      <td>4276.0</td>\n",
              "      <td>4433.0</td>\n",
              "      <td>4498.0</td>\n",
              "      <td>4877.0</td>\n",
              "      <td>5041.0</td>\n",
              "      <td>5160.0</td>\n",
              "      <td>5460.0</td>\n",
              "      <td>5795.0</td>\n",
              "      <td>5969.0</td>\n",
              "      <td>6457.0</td>\n",
              "      <td>6852.0</td>\n",
              "      <td>7245.0</td>\n",
              "      <td>7753.0</td>\n",
              "      <td>8259.0</td>\n",
              "      <td>8563.0</td>\n",
              "      <td>9075.0</td>\n",
              "      <td>9916.0</td>\n",
              "      <td>10800.0</td>\n",
              "      <td>11420.0</td>\n",
              "      <td>12543.0</td>\n",
              "      <td>13381.0</td>\n",
              "      <td>13817.0</td>\n",
              "      <td>16118.0</td>\n",
              "      <td>16545.0</td>\n",
              "      <td>16434.0</td>\n",
              "      <td>19663.0</td>\n",
              "      <td>17034.0</td>\n",
              "      <td>21174.0</td>\n",
              "      <td>17200.0</td>\n",
              "      <td>139598.0</td>\n",
              "      <td>39228.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 256 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CH1      CH2      CH3      CH4  ...    CH253    CH254     CH255    CH256\n",
              "0     3.0    285.0    331.0  66131.0  ...      0.0      0.0       4.0      0.0\n",
              "1  8553.0   3664.0   3099.0   3054.0  ...   4213.0    511.0    9524.0  31480.0\n",
              "2  5696.0  91903.0  80186.0  37287.0  ...      0.0      0.0       0.0    447.0\n",
              "3     0.0      0.0      0.0      0.0  ...   3692.0   1093.0    4477.0      0.0\n",
              "4  9582.0   8560.0   9828.0   8988.0  ...  21174.0  17200.0  139598.0  39228.0\n",
              "\n",
              "[5 rows x 256 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZL3iyqdfCRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ch_array =  ch_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ2FzhT38mW3",
        "colab_type": "text"
      },
      "source": [
        "Color Histogram - Short-term score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PysSVOSHfMrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = ch_array\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['short-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwZK7qMMjzAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5ef15d3f-dfd0-4fe6-ee69-ca8e8cdff1cf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 2000, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_AWeIHvh938",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2e32c49-d151-4110-eada-810e431dcb42"
      },
      "source": [
        "mean_squared_error(Y_test, y_pred)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021584182934185535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v6hGtwR8aVW",
        "colab_type": "text"
      },
      "source": [
        "Color Histogram Long-term Memorability "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fbCgljE8ZyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = ch_array\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['long-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECB_M5iLfHPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e3370b2c-c2ed-4767-aed9-3f5872be4384"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 800, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ9CCprV5tiq",
        "colab_type": "text"
      },
      "source": [
        "### A Test based on Gestalt's Principle - Color Histogram and Aesthetics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GgDUp636NNn",
        "colab_type": "text"
      },
      "source": [
        "Short-term memorability test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ9x0CSV5s8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color_aes = np.concatenate((ch_array, aes_array), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn9QITxd6KxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = color_aes\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['short-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa6s8tpU6Wvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d4454c0e-9342-4bc4-f903-60adc491be91"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 800, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rGoPYoqiXbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1f442af-aeab-417b-88d1-1dc6d1f4e14d"
      },
      "source": [
        "mean_squared_error(Y_test, y_pred)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005702493495975153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGKMpbsW6j7U",
        "colab_type": "text"
      },
      "source": [
        "Long - Term memorability Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzUGEAad6nUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = color_aes\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['long-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2w5vREF6oLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b8361e80-46fd-4b18-bc5f-c40ab2221e12"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 800, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mH2xvYJiaG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eb8635f-b5c9-4c46-f392-57116d875dc8"
      },
      "source": [
        "mean_squared_error(Y_test, y_pred)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021331769389618702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dxLdVwZyfTQ",
        "colab_type": "text"
      },
      "source": [
        "## Histogram of Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3w--7YkykKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Method to read HOG'''\n",
        "def read_HOG(fpath, fname, HOG_map):\n",
        "  with open(fpath) as f:\n",
        "    value_list = []\n",
        "    for line in f:\n",
        "      pairs=line.split(' ')\n",
        "      for value in pairs:\n",
        "        value_list.append(value)\n",
        "    HOG_map[fname] = value_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUFYqU-yyywz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_list = []\n",
        "for i in range(1, 7494):\n",
        "  name = 'video' + str(i)\n",
        "  filename = name + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/HOG/' + filename\n",
        "  if(os.path.exists(file_path)):\n",
        "    video_list.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdRsmDU1S_V",
        "colab_type": "code",
        "outputId": "d9187c2a-fa56-4738-f939-e7e750795a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(video_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9DSHpYnzDLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_map = {}\n",
        "#hmp_arr = np.empty((0,6075), float)\n",
        "# load the captions\n",
        "for filename in video_list:\n",
        "  filename = filename + '-56.txt'\n",
        "  file_path = './Assignment/Dev-set/HOG/' + filename\n",
        "  read_HOG(file_path, filename, HOG_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkhFp9VzSbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_df = pd.DataFrame.from_dict(HOG_map,  orient='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJLMxEfxCCTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HliwY9Rzgrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_df.reset_index(inplace=True)\n",
        "HOG_df.drop(columns=['index'], inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOi_9Ile72Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_df.to_csv('/content/drive/My Drive/HOG_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SieYnacNlQlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cbe97917-6faf-4a30-8d4a-e6db88821507"
      },
      "source": [
        "HOG_df = pd.read_csv('HOG_df.csv')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (971,1863,1951,2121,2494,2495,2506,2561,2564,2580,2835,2840,2848,2892,2903,3005,3042,3044,3086,3108,3149,3151,3161,3167,3182,3234,3260,3278,3293,3305,3383,3396,3400,3417,3438,3450,3467,3469,3477,3481,3535,3549,3552,3567,3585,3650,3655,3663,3675,3691,3736,3749,3765,3815,3863,3900,3934,3937,3943,3950,3959,3970,3977,4004,4029,4031,4056,4059,4075,4106,4115,4152,4170,4177,4186,4190,4203,4236,4247,4264,4269,4280,4291,4294,4308,4315,4369,4371,4374,4399,4403,4415,4447,4451,4453,4457,4466,4471,4473,4474,4479,4496,4501,4509,4524,4529,4530,4532,4540,4549,4566,4569,4576,4608,4626,4628,4630,4645,4650,4651,4652,4655,4679,4684,4689,4702,4705,4713,4714,4726,4728,4730,4738,4746,4748,4749,4755,4759,4761,4767,4769,4786,4792,4796,4799,4808,4809,4813,4817,4821,4824,4829,4833,4834,4837,4843,4846,4853,4856,4868,4870,4876,4880,4910,4912,4930,4931,4946,4948,4950,4960,4966,4969,4970,4978,4981,4983,4986,4987,4991,4992,4994,4995,4997,5013,5022,5025,5028,5031,5038,5049,5055,5056,5061,5064,5069,5101,5104,5108,5116,5133,5142,5147,5156,5157,5159,5165,5174,5176,5187,5197,5199,5201,5203,5216,5239,5252,5262,5263,5276,5278,5280,5284,5285,5294,5309,5310,5313,5318,5326,5331,5335,5342,5344,5363,5367,5370,5371,5388,5394,5408,5411,5415,5419,5420,5438,5440,5453,5462,5469,5472,5479,5481,5482,5488,5489,5490,5505,5510,5516,5520,5523,5533,5536,5543,5550,5566,5567,5570,5572,5574,5575,5578,5579,5597,5612,5613,5614,5615,5618,5623,5630,5641,5642,5650,5657,5671,5678,5682,5714,5717,5721,5724,5728,5729,5731,5733,5737,5746,5749,5750,5751,5752,5754,5770,5771,5774,5777,5782,5797,5804,5807,5813,5820,5825,5830,5832,5835,5844,5854,5858,5863,5865,5867,5877,5879,5881,5886,5901,5902,5903,5908,5912,5918,5937,5944,5947,5948,5949,5951,5952,5965,5978,5980,5983,6001,6013,6024,6025,6031,6035,6037,6038,6039,6049,6052,6055,6074,6075,6078,6080,6084,6087,6088,6093,6095,6097,6106,6109,6119,6124,6131,6133,6139,6143,6149,6152,6174,6180,6194,6197,6200,6207,6212,6216,6218,6219,6224,6228,6233,6236,6243,6248,6251,6266,6269,6270,6273,6274,6276,6277,6278,6280,6291,6294,6296,6302,6313,6316,6317,6319,6323,6324,6327,6330,6331,6332,6340,6341,6342,6363,6372,6379,6384,6393,6399,6400,6404,6406,6407,6408,6413,6418,6421,6432,6434,6436,6439,6440,6444,6457,6459,6461,6468,6469,6474,6477,6483,6484,6486,6488,6502,6513,6514,6517,6525,6529,6530,6537,6539,6540,6550,6552,6554,6556,6567,6575,6576,6582,6587,6588,6592,6593,6594,6603,6606,6608,6613,6614,6623,6624,6634,6639,6640,6644,6647,6661,6664,6666,6670,6671,6672,6673,6675,6680,6699,6703,6706,6712,6722,6725,6730,6734,6737,6739,6740,6741,6745,6747,6752,6754,6758,6759,6761,6765,6770,6777,6782,6784,6788,6797,6808,6813,6814,6817,6822,6827,6829,6834,6836,6840,6848,6856,6860,6866,6867,6869,6871,6875,6876,6877,6888,6891,6894,6896,6901,6905,6906,6907,6908,6911,6912,6914,6915,6932,6935,6937,6939,6941,6942,6947,6954,6957,6960,6962,6969,6972,6976,6980,6981,6984,6986,6987,6988,6993,7007,7012,7014,7015,7020,7021,7022,7028,7032,7034,7035,7037,7038,7039,7040,7041,7044,7045,7046,7047,7052,7054,7068,7074,7078,7083,7089,7092,7093,7096,7103,7110,7114,7120,7126,7127,7128,7130,7133,7138,7143,7144,7155,7157,7159,7168,7169,7172,7174,7184,7185,7192,7198,7199,7203,7214,7215,7220,7227,7231,7232,7233,7234,7237,7241,7242,7249,7255,7257,7268,7270,7281,7282,7284,7291,7294,7296,7299,7305,7306,7307,7312,7317,7321,7325,7328,7329,7331,7334,7337,7342,7349,7350,7352,7363,7370,7372,7374,7375,7376,7381,7385,7390,7396,7397,7401,7404,7406,7414,7416,7422,7423,7435,7437,7439,7443,7448,7452,7456,7457,7459,7460,7461,7466,7473,7476,7477,7482,7484,7486,7491,7492,7496,7497,7499,7500,7502,7503,7505,7510,7523,7528,7535,7540,7545,7547,7552,7557,7562,7566,7571,7572,7573,7576,7578,7579,7581,7582,7584,7585,7587,7589,7606,7607,7608,7610,7611,7613,7615,7616,7618,7620,7623,7634,7635,7639,7642,7644,7648,7649,7652,7655,7656,7659,7666,7667,7668,7674,7676,7677,7679,7680,7681,7685,7686,7690,7695,7698,7700,7705,7706,7707,7720,7721,7726,7728,7730,7732,7733,7734,7736,7741,7742,7746,7750,7752,7756,7760,7761,7762,7765,7766,7774,7775,7776,7782,7783,7784,7791,7796,7804,7806,7807,7808,7810,7813,7818,7821,7823,7827,7828,7829,7832,7834,7837,7842,7843,7844,7850,7851,7852,7853,7854,7855,7856,7861,7864,7866,7867,7869,7870,7873,7874,7875,7877,7882,7887,7893,7896,7897,7899,7900,7903,7905,7911,7914,7915,7916,7928,7929,7930,7932,7934,7945,7949,7950,7952,7953,7956,7958,7959,7967,7968,7973,7975,7981,7983,7984,7985,7987,7988,7990,7992,7996,7998,8001,8002,8004,8008,8021,8022,8024,8025,8026,8029,8032,8034,8036,8040,8041,8048,8049,8050,8051,8054,8058,8061,8062,8067,8068,8069,8073,8074,8075,8076,8078,8080,8082,8085,8088,8090,8091,8095,8096,8097,8101,8103,8104,8105,8109,8110,8112,8113,8114,8115,8116,8120,8121,8122,8123,8126,8131,8134,8135,8138,8141,8147,8148,8150,8151,8154,8155,8156,8159,8162,8165,8168,8170,8173,8175,8176,8178,8179,8180,8181,8185,8187,8188,8189,8191,8192,8194,8195,8196,8198,8199,8201,8203,8204,8205,8206,8207,8208,8211,8219,8220,8221,8222,8223,8224,8226,8227,8230,8231,8234,8236,8242,8243,8246,8247,8249,8250,8252,8256,8257,8258,8259,8260,8262,8271,8275,8281,8285,8287,8288,8291,8295,8300,8302,8304,8306,8307,8309,8310,8312,8313,8317,8321,8322,8323,8324,8327,8329,8333,8334,8336,8337,8340,8341,8342,8343,8344,8345,8347,8348,8350,8353,8354,8355,8360,8364,8366,8368,8370,8372,8373,8374,8375,8380,8384,8385,8388,8389,8392,8396,8397,8398,8399,8401,8403,8405,8409,8411,8412,8414,8416,8420,8421,8423,8425,8427,8429,8430,8432,8434,8437,8438,8439,8440,8441,8442,8444,8447,8448,8449,8451,8455,8456,8458,8459,8461,8464,8465,8467,8468,8469,8472,8476,8477,8479,8480,8481,8482,8484,8485,8487,8488,8490,8492,8495,8500,8501,8502,8504,8505,8506,8507,8510,8512,8514,8516,8518,8519,8521,8522,8523,8524,8525,8527,8529,8532,8534,8539,8540,8541,8545,8546,8549,8550,8553,8554,8555,8556,8557,8558,8559,8560,8561,8562,8563,8564,8565,8567,8572,8574,8576,8577,8578,8581,8582,8585,8589,8591,8592,8594,8598,8599,8600,8603,8605,8611,8612,8614,8615,8619,8621,8622,8627,8628,8630,8631,8633,8636,8637,8639,8640,8642,8644,8645,8647,8648,8655,8657,8661,8662,8670,8671,8673,8674,8675,8676,8677,8678,8679,8682,8684,8685,8686,8694,8695,8696,8699,8700,8702,8704,8705,8706,8709,8710,8711,8712,8719,8720,8723,8724,8725,8727,8728,8729,8731,8732,8733,8734,8738,8740,8742,8745,8746,8748,8754,8756,8757,8761,8765,8768,8772,8775,8777,8784,8785,8786,8789,8792,8793,8794,8796,8797,8798,8799,8803,8805,8807,8808,8809,8814,8817,8818,8821,8822,8823,8825,8827,8829,8831,8832,8834,8837,8839,8841,8842,8843,8844,8845,8847,8848,8849,8851,8852,8854,8857,8860,8861,8863,8864,8867,8869,8870,8874,8875,8876,8877,8878,8881,8882,8883,8884,8885,8887,8889,8891,8892,8893,8894,8896,8898,8902,8904,8905,8908,8909,8911,8912,8913,8914,8915,8918,8919,8927,8928,8929,8930,8931,8933,8935,8936,8938,8940,8941,8943,8944,8945,8949,8952,8953,8954,8955,8956,8957,8958,8959,8963,8964,8966,8967,8968,8969,8971,8972,8974,8975,8978,8979,8980,8981,8983,8986,8987,8988,8989,8991,8993,8994,8995,8996,8997,8999,9002,9003,9004,9005,9010,9011,9012,9013,9014,9015,9018,9019,9020,9021,9022,9023,9024,9026,9027,9030,9032,9033,9034,9035,9038,9039,9040,9041,9042,9043,9044,9045,9048,9050,9053,9054,9056,9058,9062,9063,9064,9065,9067,9068,9069,9071,9072,9074,9076,9079,9082,9084,9085,9086,9087,9088,9089,9090,9092,9094,9095,9099,9100,9101,9103,9104,9105,9106,9107,9108,9109,9110,9111,9116,9117,9119,9120,9125,9126,9131,9133,9134,9135,9137,9138,9139,9140,9144,9145,9146,9147,9149,9154,9155,9156,9157,9161,9166,9167,9171,9172,9174,9175,9176,9178,9180,9184,9185,9186,9187,9188,9189,9190,9191,9192,9193,9194,9195,9197,9198,9201,9202,9203,9204,9205,9209,9211,9216,9217,9219,9221,9223,9224,9225,9226,9227,9228,9229,9231,9233,9234,9235,9236,9237,9238,9240,9243,9244,9246,9249,9250,9251,9252,9254,9255,9256,9257,9259,9262,9264,9266,9267,9269,9270,9271,9272,9273,9274,9275,9276,9278,9279,9281,9282,9283,9284,9285,9286,9287,9289,9290,9291,9292,9293,9294,9295,9296,9298,9300,9308,9311,9312,9319,9320,9322,9323,9324,9326,9327,9329,9331,9336,9337,9339,9340,9342,9343,9346,9348,9350,9351,9352,9353,9354,9356,9358,9359,9360,9363,9367,9369,9371,9373,9374,9375,9377,9378,9379,9380,9383,9386,9387,9389,9390,9392,9394,9396,9397,9398,9399,9400,9403,9404,9405,9406,9407,9409,9410,9412,9413,9414,9417,9418,9419,9421,9424,9425,9427,9429,9431,9432,9433,9436,9439,9440,9442,9443,9445,9446,9447,9448,9451,9455,9456,9457,9459,9460,9462,9466,9467,9468,9470,9471,9472,9474,9475,9477,9480,9481,9483,9484,9485,9488,9490,9491,9492,9494,9495,9496,9497,9498,9499,9500,9502,9503,9504,9506,9507,9508,9509,9510,9511,9512,9513,9515,9516,9517,9520,9521,9523,9524,9525,9526,9527,9528,9530,9531,9532,9533,9535,9538,9539,9540,9543,9544,9545,9546,9547,9548,9549,9550,9551,9554,9555,9556,9561,9563,9564,9565,9566,9567,9568,9569,9571,9572,9574,9575,9576,9577,9578,9579,9581,9582,9583,9584,9585,9586,9588,9591,9592,9596,9597,9600,9602,9604,9605,9606,9607,9609,9610,9614,9615,9616,9617,9618,9619,9621,9622,9623,9625,9626,9627,9629,9631,9632,9633,9635,9636,9637,9638,9639,9641,9642,9643,9644,9645,9647,9648,9649,9650,9651,9652,9653,9655,9656,9657,9659,9660,9663,9664,9668,9669,9670,9671,9672,9674,9676,9677,9681,9682,9683,9684,9685,9687,9689,9690,9691,9692,9693,9694,9695,9696,9697,9698,9699,9701,9702,9704,9705,9706,9707,9710,9712,9714,9715,9719,9720,9721,9723,9724,9725,9726,9727,9728,9730,9731,9732,9733,9734,9735,9737,9738,9739,9741,9742,9743,9744,9745,9746,9748,9749,9750,9753,9754,9756,9758,9759,9760,9761,9762,9765,9766,9767,9768,9771,9772,9775,9776,9777,9778,9779,9780,9781,9782,9783,9784,9785,9786,9787,9789,9791,9792,9794,9795,9797,9798,9800,9802,9803,9804,9805,9807,9809,9810,9811,9812,9815,9816,9817,9818,9819,9820,9821,9823,9824,9825,9829,9830,9831,9832,9833,9834,9835,9836,9837,9838,9839,9840,9841,9842,9844,9845,9847,9848,9850,9851,9852,9855,9856,9859,9860,9861,9862,9863,9865,9867,9868,9869,9870,9871,9872,9874,9875,9876,9877,9878,9879,9880,9881,9883,9885,9886,9888,9889,9890,9893,9897,9898,9899,9900,9901,9902,9903,9907,9908,9909,9912,9913,9915,9917,9919,9920,9921,9922,9924,9926,9927,9928,9933,9937,9940,9941,9943,9945,9947,9948,9949,9952,9953,9954,9955,9956,9957,9959,9961,9962,9963,9964,9965,9966,9967,9968,9969,9970,9971,9972,9973,9974,9975,9977,9978,9979,9980,9981,9982,9985,9987,9988,9989,9990,9991,9992,9993,9995,9998,9999,10000,10001,10002,10003,10004,10006,10008,10009,10010,10012,10013,10014,10015,10016,10017,10018,10019,10020,10021,10022,10025,10026,10027,10029,10030,10034,10036,10039,10041,10043,10046,10047,10048,10049,10051,10052,10053,10054,10055,10056,10059,10061,10062,10064,10065,10069,10070,10071,10072,10073,10074,10075,10076,10077,10078,10079,10080,10081,10082,10084,10085,10086,10087,10089,10090,10091,10092,10093,10094,10096,10097,10098,10099,10100,10101,10102,10103,10106,10107,10108,10109,10111,10112,10113,10114,10115,10117,10118,10119,10120,10121,10122,10123,10124,10125,10126,10127,10128,10129,10130,10131,10135,10138,10139,10140,10141,10142,10144,10146,10147,10149,10151,10152,10154,10156,10158,10159,10164,10166,10167,10168,10169,10171,10172,10173,10174,10175,10176,10177,10181,10183,10184,10185,10186,10187,10188,10189,10190,10192,10193,10195,10196,10197,10198,10199,10200,10201,10202,10205,10206,10207,10208,10209,10212,10215,10217,10218,10219,10220,10221,10223,10224,10226,10227,10228,10231,10233,10234,10235,10236,10237,10238,10239,10240,10241,10242,10246,10247,10248,10249,10250,10251,10252,10253,10254,10256,10259,10260,10262,10263,10264,10266,10268,10269,10270,10272,10273,10275,10276,10278,10279,10280,10281,10283,10284,10285,10286,10289,10290,10293,10294,10295,10296,10297,10298,10299,10300,10301,10302,10303,10304,10305,10307,10308,10309,10311,10312,10313,10314,10316,10317,10318,10319,10321,10322,10323,10325,10327,10328,10329,10330,10331,10332,10334,10335,10336,10337,10338,10339,10340,10342,10343,10345,10347,10348,10350,10351,10352,10353,10354,10356,10357,10358,10359,10360,10361,10362,10363,10364,10365,10366,10367,10368,10370,10371,10373,10374,10375,10378,10380,10381,10382,10383,10384,10385,10386,10387,10388,10389,10391,10396,10397,10399,10400,10401,10402,10403,10404,10405,10406,10407,10408,10409,10410,10411,10413,10414,10415,10416,10417,10418,10419,10420,10421,10422,10425,10426,10427,10428,10429,10430,10431,10432,10433,10434,10435,10436,10437,10438,10439,10441,10442,10445,10446,10447,10448,10450,10451,10452,10453,10454,10455,10456,10457,10458,10459,10461,10462,10464,10465,10466,10467,10468,10469,10470,10471,10473,10474,10475,10476,10477,10478,10479,10480,10481,10482,10484,10485,10487,10488,10489,10490,10491,10492,10493,10494,10495,10496,10497,10498,10500,10501,10503,10504,10505,10506,10507,10508,10509,10510,10511,10512,10513,10515,10516,10517,10518,10519,10520,10521,10522,10523,10525,10526,10527,10528,10529,10530,10531,10533,10535,10536,10537,10538,10539,10540,10541,10543,10546,10548,10549,10550,10552,10553,10554,10555,10556,10557,10558,10560,10561,10562,10563,10564,10565,10566,10567,10568,10569,10570,10571,10572,10574,10575,10576,10577,10580,10581,10582,10583,10584,10585,10586,10587,10588,10589,10590,10591,10592,10593,10594,10595,10597,10600,10601,10602,10604,10605,10606,10608,10609,10610,10612,10613,10616,10617,10618,10620,10621,10622,10624,10625,10626,10627,10628,10629,10631,10633,10635,10636,10637,10638,10639,10640,10641,10642,10643,10644,10646,10648,10649,10650,10654,10656,10657,10659,10660,10662,10663,10664,10665,10666,10667,10668,10670,10671,10672,10673,10674,10675,10677,10678,10679,10680,10681,10682,10684,10685,10686,10687,10688,10689,10690,10691,10692,10693,10694,10695,10698,10700,10701,10702,10704,10707,10708,10709,10710,10711,10712,10713,10714,10715,10716,10717,10718,10719,10720,10721,10722,10723,10724,10725,10726,10728,10729,10730,10731,10732,10734,10735,10736,10737,10738,10739,10740,10741,10743,10744,10745,10746,10747,10748,10749,10751,10752,10753,10754,10755,10756,10758,10759,10761,10763,10764,10765,10766,10767,10768,10771,10772,10773,10774,10775,10776,10777,10778,10779,10780,10781,10782,10783,10784,10786,10787,10789,10790,10791,10792,10794,10795,10796,10797,10798,10799,10800,10801,10802,10803,10804,10805,10806,10807,10810,10811,10814,10815,10817,10818,10819,10820,10821,10822,10823,10824,10825,10826,10827,10828,10829,10831,10833,10835,10836,10839,10841,10842,10843,10844,10846,10847,10849,10850,10851,10852,10854,10855,10856,10857,10858,10859,10860,10861,10867,10868,10870,10871,10872,10873,10875,10877,10878,10879,10880,10881,10882,10884,10885,10886,10887,10888,10889,10890,10891,10892,10893,10894,10895,10896,10897,10899,10902,10903,10904,10905,10907,10908,10909,10910,10911,10912,10913,10914,10915,10918,10919,10921,10922,10923,10924,10925,10926,10927,10928,10929,10930,10931,10932,10933,10934,10935,10936,10938,10940,10941,10942,10943,10944,10945,10946,10947,10949,10950,10951,10953,10955,10956,10958,10959,10961,10962,10963,10964,10965,10966,10967,10968,10969,10971,10972,10973,10974,10975,10976,10977,10978,10980,10981,10982,10983,10984,10985,10986,10987,10988,10990,10991,10992,10993,10995,10996,10997,10999,11000,11003,11005,11006,11007,11008,11010,11011,11012,11013,11014,11018,11020,11021,11022,11023,11024,11025,11026,11027,11028,11029,11030,11031,11032,11033,11034,11035,11036,11037,11038,11039,11040,11041,11042,11045,11046,11047,11049,11050,11051,11052,11053,11055,11056,11059,11060,11061,11062,11063,11065,11066,11068,11069,11071,11072,11073,11074,11076,11077,11078,11079,11080,11081,11082,11083,11084,11086,11088,11090,11091,11092,11093,11094,11095,11096,11100,11101,11102,11103,11104,11105,11106,11107,11109,11110,11111,11112,11113,11114,11115,11117,11118,11119,11121,11122,11125,11126,11127,11128,11129,11130,11131,11133,11134,11136,11137,11138,11139,11140,11142,11143,11145,11146,11147,11148,11149,11150,11151,11152,11153,11155,11156,11157,11158,11159,11160,11161,11165,11166,11167,11169,11170,11172,11173,11174,11175,11176,11177,11178,11179,11180,11181,11182,11183,11185,11186,11187,11188,11189,11190,11191,11192,11193,11194,11195,11196,11197,11198,11199,11200,11202,11206,11208,11209,11210,11211,11212,11213,11214,11215,11216,11218,11219,11220,11221,11224,11225,11226,11227,11228,11229,11230,11231,11232,11233,11234,11235,11236,11237,11238,11239,11240,11241,11242,11243,11244,11246,11247,11248,11249,11250,11251,11252,11254,11255,11256,11258,11260,11261,11262,11263,11264,11265,11266,11268,11272,11273,11274,11275,11277,11279,11280,11281,11282,11284,11286,11287,11288,11289,11290,11291,11292,11293,11294,11296,11297,11298,11300,11302,11304,11305,11306,11307,11308,11309,11310,11311,11312,11313,11314,11316,11317,11318,11319,11320,11321,11324,11328,11329,11330,11331,11332,11333,11334,11338,11339,11341,11342,11343,11344,11345,11347,11348,11350,11351,11353,11354,11355,11357,11358,11359,11360,11361,11362,11363,11365,11366,11368,11370,11371,11372,11373,11374,11376,11377,11378,11379,11382,11383,11384,11385,11387,11389,11390,11391,11392,11395,11398,11399,11400,11401,11402,11403,11404,11405,11406,11407,11408,11409,11410,11412,11413,11415,11418,11420,11421,11422,11423,11424,11425,11426,11427,11429,11430,11431,11432,11433,11434,11435,11436,11438,11439,11440,11444,11445,11446,11447,11448,11449,11450,11451,11452,11454,11455,11457,11459,11460,11461,11465,11466,11467,11469,11470,11471,11472,11474,11475,11476,11478,11479,11480,11482,11483,11484,11485,11486,11487,11488,11489,11490,11491,11492,11495,11496,11497,11498,11499,11500,11501,11503,11504,11507,11510,11511,11513,11515,11516,11517,11519,11520,11521,11522,11523,11525,11527,11529,11530,11531,11532,11534,11535,11536,11537,11538,11539,11542,11546,11547,11548,11549,11550,11551,11552,11554,11556,11557,11559,11562,11563,11564,11565,11566,11572,11574,11575,11576,11578,11579,11582,11583,11584,11585,11586,11587,11590,11592,11593,11595,11597,11598,11599,11600,11601,11602,11603,11604,11605,11606,11607,11608,11609,11610,11611,11612,11614,11616,11617,11618,11620,11622,11624,11626,11627,11628,11629,11631,11633,11634,11635,11636,11639,11640,11641,11646,11647,11648,11649,11650,11651,11653,11654,11655,11656,11657,11659,11660,11661,11662,11666,11667,11668,11669,11671,11672,11674,11675,11676,11677,11680,11681,11684,11685,11688,11689,11690,11691,11692,11693,11695,11696,11698,11699,11700,11702,11703,11704,11705,11706,11707,11709,11710,11711,11712,11713,11714,11715,11716,11717,11720,11721,11722,11723,11724,11725,11726,11727,11730,11731,11732,11733,11734,11735,11736,11737,11738,11740,11742,11744,11746,11747,11748,11749,11752,11753,11756,11757,11761,11766,11767,11770,11771,11772,11773,11775,11776,11778,11779,11780,11781,11782,11783,11784,11785,11786,11787,11789,11791,11792,11793,11795,11796,11797,11801,11806,11807,11809,11810,11813,11816,11817,11818,11819,11824,11825,11826,11827,11828,11830,11831,11832,11834,11835,11837,11839,11840,11841,11842,11843,11845,11850,11852,11855,11857,11858,11859,11861,11862,11864,11865,11868,11869,11872,11873,11874,11875,11877,11879,11882,11885,11887,11888,11889,11890,11892,11894,11895,11896,11897,11898,11899,11900,11902,11905,11906,11907,11908,11910,11913,11916,11919,11920,11925,11926,11929,11931,11932,11937,11938,11939,11940,11942,11944,11945,11946,11947,11948,11949,11953,11954,11955,11959,11960,11963,11964,11965,11967,11973,11976,11977,11982,11983,11984,11985,11986,11988,11989,11994,11999,12000,12004,12006,12009,12010,12014,12015,12018,12020,12021,12022,12027,12028,12029,12030,12031,12032,12034,12035,12036,12038,12039,12040,12042,12043,12049,12050,12051,12053,12054,12055,12058,12060,12062,12064,12068,12072,12075,12076,12077,12078,12083,12085,12088,12089,12090,12096,12097,12098,12099,12103,12104,12105,12110,12114,12115,12117,12118,12120,12121,12126,12127,12128,12129,12133,12135,12139,12143,12146,12147,12148,12149,12150,12155,12157,12159,12162,12168,12169,12171,12172,12173,12174,12176,12184,12186,12187,12190,12192,12196,12197,12204,12205,12207,12208,12211,12214,12215,12216,12221,12222,12223,12224,12225,12227,12231,12235,12236,12237,12240,12246,12249,12251,12253,12254,12255,12264,12267,12268,12272,12276,12279,12281,12284,12285,12287,12295,12296,12299,12303,12310,12311,12313,12319,12327,12329,12331,12338,12340,12343,12344,12345,12353,12355,12358,12359,12361,12364,12369,12371,12378,12384,12385,12387,12390,12391,12395,12398,12399,12404,12409,12410,12412,12413,12420,12421,12429,12430,12434,12436,12437,12441,12443,12448,12451,12452,12459,12460,12464,12465,12468,12469,12470,12472,12474,12475,12477,12483,12485,12489,12492,12493,12495,12496,12497,12503,12504,12513,12514,12518,12520,12521,12526,12529,12530,12536,12543,12551,12552,12553,12558,12560,12561,12565,12568,12569,12570,12573,12575,12580,12582,12586,12587,12591,12593,12595,12605,12608,12609,12610,12613,12615,12619,12625,12628,12632,12634,12635,12636,12639,12640,12645,12652,12654,12656,12657,12659,12663,12667,12668,12670,12673,12676,12687,12692,12693,12696,12700,12707,12708,12721,12725,12727,12732,12741,12748,12753,12756,12760,12768,12775,12792,12795,12803,12807,12808,12819,12822,12823,12826,12828,12839,12842,12847,12849,12856,12862,12875,12892,12897,12899,12900,12911,12912,12924,12926,12927,12943,12950,12952,12966,12974,12980,12988,12998,13008,13010,13013,13031,13036,13038,13041,13044,13048,13052,13070,13071,13074,13078,13079,13083,13094,13095,13098,13101,13109,13117,13118,13140,13150,13154,13179,13184,13191,13192,13193,13199,13200,13203,13213,13235,13236,13252,13255,13283,13299,13303,13311,13312,13321,13329,13343,13347,13352,13356,13363,13370,13385,13413,13431,13442,13443,13461,13465,13468,13474,13490,13578,13603,13657,13669,13676,13709,13714,13734,13816,13831,13839,13876,13909,13927,13947,13951,13967,13999,14016,14026,14066,14073,14095,14125,14140,14150,14193,14206,14209,14220,14234,14294,14305,14324,14360,14457,14499,14536,14543,14637,14639,14640,14716,14740,14810,14812,14846,14934,14981,15174,15204,15462,15984) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB2ChuBjvND6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "8a6c443f-0fe9-40e8-a455-3a7f92155308"
      },
      "source": [
        "HOG_df.head()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>11676</th>\n",
              "      <th>11677</th>\n",
              "      <th>11678</th>\n",
              "      <th>11679</th>\n",
              "      <th>11680</th>\n",
              "      <th>11681</th>\n",
              "      <th>11682</th>\n",
              "      <th>11683</th>\n",
              "      <th>11684</th>\n",
              "      <th>11685</th>\n",
              "      <th>11686</th>\n",
              "      <th>11687</th>\n",
              "      <th>11688</th>\n",
              "      <th>11689</th>\n",
              "      <th>11690</th>\n",
              "      <th>11691</th>\n",
              "      <th>11692</th>\n",
              "      <th>11693</th>\n",
              "      <th>11694</th>\n",
              "      <th>11695</th>\n",
              "      <th>11696</th>\n",
              "      <th>11697</th>\n",
              "      <th>11698</th>\n",
              "      <th>11699</th>\n",
              "      <th>11700</th>\n",
              "      <th>11701</th>\n",
              "      <th>11702</th>\n",
              "      <th>11703</th>\n",
              "      <th>11704</th>\n",
              "      <th>11705</th>\n",
              "      <th>11706</th>\n",
              "      <th>11707</th>\n",
              "      <th>11708</th>\n",
              "      <th>11709</th>\n",
              "      <th>11710</th>\n",
              "      <th>11711</th>\n",
              "      <th>11712</th>\n",
              "      <th>11713</th>\n",
              "      <th>11714</th>\n",
              "      <th>11715</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.172334</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.028002</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>...</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>0.00022447</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>0.00022447</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.00011223</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.00022447</td>\n",
              "      <td>0.00011223</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.00022447</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>0.00028058</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>0.00016835</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.00011223</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.00022447</td>\n",
              "      <td>0.00011223</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>5.612e-05</td>\n",
              "      <td>0.00016835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.195286</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.222054</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 11716 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...      11713      11714       11715\n",
              "0  0.172334  0.000056  0.000056  ...        NaN        NaN         NaN\n",
              "1  0.028002  0.000056  0.000056  ...  5.612e-05  5.612e-05  0.00016835\n",
              "2  0.195286  0.000056  0.000056  ...        NaN        NaN         NaN\n",
              "3  0.222054  0.000056  0.000056  ...        NaN        NaN         NaN\n",
              "4  0.000168  0.000056  0.000056  ...        NaN        NaN         NaN\n",
              "\n",
              "[5 rows x 11716 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5wtIs7Yu7zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOG_df = HOG_df.fillna('0.0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVZ-eUo9vXu6",
        "colab_type": "text"
      },
      "source": [
        "## C3D "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5EhFnFJ7U0-",
        "colab_type": "text"
      },
      "source": [
        "### Read File from Drive and store it in a dataframe for easy readability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki5zgotLvXK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Method to read HOG'''\n",
        "def read_c3d(fpath, fname, c3d_map):\n",
        "  with open(fpath) as f:\n",
        "    value_list = []\n",
        "    for line in f:\n",
        "      pairs=line.split(' ')\n",
        "      for value in pairs:\n",
        "        value_list.append(value)\n",
        "    c3d_map[fname] = value_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSmBj3oZvluM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_list = []\n",
        "for i in range(1, 7494):\n",
        "  name = 'video' + str(i)\n",
        "  filename = name + '.txt'\n",
        "  file_path = './Assignment/Dev-set/C3D/' + filename\n",
        "  if(os.path.exists(file_path)):\n",
        "    video_list.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRmrQl-cvyJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_map = {}\n",
        "#hmp_arr = np.empty((0,6075), float)\n",
        "# load the captions\n",
        "for filename in video_list:\n",
        "  filename = filename + '.txt'\n",
        "  file_path = './Assignment/Dev-set/C3D/' + filename\n",
        "  read_c3d(file_path, filename, c3d_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdg5rP8v41t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df = pd.DataFrame.from_dict(c3d_map,  orient='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-tSnHwD9lWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zpYoVao9epw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df.to_csv('/content/drive/My Drive/c3d_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG14vU7C7Heh",
        "colab_type": "text"
      },
      "source": [
        "### **C3D - Train models for video memorability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSYv_LD2lT_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df = pd.read_csv('c3d_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSiLOiYDlkMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df = c3d_df.iloc[:, 1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZKtQHshlaMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-7l6pYNluLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c3d_arr  = c3d_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xey1_Fv2mEJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbp_c3d = np.concatenate((c3d_arr, LBP_df.values), axis=1)\n",
        "color_c3d = np.concatenate((ch_array, c3d_arr), axis=1)\n",
        "cap_c3d = np.concatenate((one_hot_senti, c3d_arr), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Nqfz4v7f9Z",
        "colab_type": "text"
      },
      "source": [
        "Short-term memorability Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYn2waEPlytz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = c3d_arr\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['short-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIYWC6qt8Akr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 2000, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsBsJHLm8Gcd",
        "colab_type": "text"
      },
      "source": [
        "Long-term memorability score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGSIsLP7-HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = c3d_arr\n",
        "#Y = labels[['short-term_memorability','long-term_memorability']].values\n",
        "Y = labels[['long-term_memorability']].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS6UyNi8l4qG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1f59127c-2a1b-4ade-fe2e-3409cbab11c1"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "clf = RandomForestRegressor(n_estimators = 2000, max_features= 4, min_samples_leaf = 5, min_samples_split= 8)\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "Get_score(y_pred, Y_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPpiXyN79hG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtBLQJTy9jS9",
        "colab_type": "text"
      },
      "source": [
        "## Histogram of Moving Patterns"
      ]
    }
  ]
}